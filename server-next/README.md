# Indexify 

#### Stateful Compute Engine for building and deploying LLM Workflows and Applications

Indexify is a stateful compute engine that empowers you to build and deploy multi-stage LLM workflows and applications as live API endpointsâ€”using plain Python. 

* Write workflows **without handling state preservation** during retries, traffic overloads, or managing intermediate results.

* **Distribute** workflows across thousands of machines for parallel execution.

* **Place functions on appropriate hardware:** Run large model-invoking functions on GPUs while executing application logic or data fetching on cost-effective CPUs.

* **Automatic Batching:** Indexify automatically batches parallel workflow invocations and executes them on GPU machines, maximizing the utilization of your most valuable resources.

Workflows are structured as Graphs, enabling many interesting use-cases -

- **Document Processing and Indexing Pipelines**

- **Audio Transcription, Summarization and Indexing APIs**

- **Web Scraping and Structured Extraction Pipelines**

- **Multi-Stage-Retrieval APIs for Advanced RAG**


## Install 
```bash
pip install indexify
```

## Basic Usage 

Workflows are written as Python functions and laid out as Graphs. Functions in a Graph are automatically invoked when upstream functions finishes with their output.

API calls to workflows are automatically queued, failures are retried so you don't have to install any other tools like Kafka, SQS or databases and write code to manage state of your Graph API endpoints.

#### Write a Workflow 
```python
from pydantic import BaseModel

class Sum(BaseModel):
    val: int

@indexify_function(retries=3)
def generate_sequence(a: int) -> List[int]:
    return [i for i in range(a)]

@indexify_function(init_value=Sum)
def sum_all_numbers(sum: Sum, val: int) -> Sum:
    return Sum(sum.val + val)

@indexify_function(placement_constraints="python_ver>3.8 and os=linux")
def display(sum: Sum) -> str:
    return f"value of sequence: f{sum.val}"

from indexify import ComputeGraph
g = ComputeGraph(name="sequence_summer", start_node=generate_sequence, description="Simple Sequence Summer")
g.add_edge(generate_sequence, sum_all_numbers)
g.add_edge(sum_all_numbers, display)
```

#### Register and Invoke the Compute Graph 
```python
from indexify import create_client 
client = create_client(ephemeral=True)
client.register_compute_graph(g)

client.invoke_workflow_with_object("sequence_summer", a=10)
result = client.graph_outputs("sequence_summer", "display")
print(result)
```

This is it! You have built and your first multi-stage workflow locally! 

#### Create a Graph Endpoint HTTP API  

Indexify comes with a server for deploying Compute Graphs as an API so they can be called from other applications or systems.

```bash
indexify-cli run-server
```

This starts the Indexify Server and an Executor - 

**Indexify Server**: Manages state of your graphs when they are called. It stores the outputs of the functions and calls them based on the structure of the graph. 

**Executor**: Runs the python functions which are part of your Graph.

Change the code above to deploy the graph as an API on the server -

```python
client = create_client() # Remove ephemeral=True
client.register_compute_graph(g) # Same as above
```

This serializes your Graph code and uploads it to the server, and instantiates a new endpoint.

Everything else, remains the same in your application code that invokes the Graph to process data and retrieve outputs! 

**What happens when you invoke a Compute Graph API?**

* Indexify serializes the input and calls the API over HTTP. 

* The server creates and schedules a Task for the fist function on an executor.

* The executor loads and executes the function and sends the data back to the server.

* The two above steps are executed for every function in the Graph. 

#### Distributed Execution by Design 

**Parallel Execution** - You can run as many executors you want on a single machine or on many 100s of machines. This will enable Indexify Server to run your graphs in parallel if they are invoked 1000s of times concurrently. 

**Placement Constraints** - You can make functions on the graph be executed on different classes of machines.

## Programming Model 

#### Automatic Parallelization 

When a function returns a `List` the downstream function is automatically called in parallel with all the elements of the list.

```python
def func_a(..) -> List[int]:
    pass

def func_b(..) -> SomeValue:
    pass
```

In the above example calls to `func_b` will be automatically parallelized from the outputs of `func_a`. 

**Use Cases:** - Generating Embedding from every single chunk of a workflow.

#### Reducing/Accumulating from Sequences

```python
def func_a(..) -> List[int]:
    pass

class SomeValue:
    foo

def func_b(val: SomeValue, ..) -> SomeValue:
    pass
```

In this example, `func_b` will be called serially for every value generated by `func_a`, and the previous `SomeValue` returned will 
be injected in the function. This allows to incrementally build state from a sequence generated by upstream functions.

**Use Cases:** - Producing a single summary from scraping 100s of web pages on a specific topic.
