{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6751dae7-4f17-4d4f-9b36-cc57915806d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pydantic in /Users/diptanuc/Projects/indexify-extractors/ve/lib/python3.12/site-packages (2.7.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /Users/diptanuc/Projects/indexify-extractors/ve/lib/python3.12/site-packages (from pydantic) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.18.1 in /Users/diptanuc/Projects/indexify-extractors/ve/lib/python3.12/site-packages (from pydantic) (2.18.1)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /Users/diptanuc/Projects/indexify-extractors/ve/lib/python3.12/site-packages (from pydantic) (4.10.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install pydantic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c62996ab-48fb-4a63-a7de-7223ddb61f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "class Extraction(BaseModel):\n",
    "    topic: str\n",
    "    summary: str\n",
    "    hypothetical_questions: List[str] = Field(\n",
    "        default_factory=list,\n",
    "        description=\"Hypothetical questions that this document could answer\",\n",
    "    )\n",
    "    keywords: List[str] = Field(\n",
    "        default_factory=list, description=\"Keywords that this document is about\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c3197573-7d6a-4741-82fc-4ba9fd15674a",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_chunk = \"\"\"\n",
    "## Simple RAG\n",
    "\n",
    "****What is it?****\n",
    "\n",
    "The simplest implementation of RAG embeds a user query and do a single embedding search in a vector database, like a vector store of Wikipedia articles. However, this approach often falls short when dealing with complex queries and diverse data sources.\n",
    "\n",
    "**What are the limitations?**\n",
    "\n",
    "- **Query-Document Mismatch:** It assumes that the query and document embeddings will align in the vector space, which is often not the case.\n",
    "    - Query: \"Tell me about climate change effects on marine life.\"\n",
    "    - Issue: The model might retrieve documents related to general climate change or marine life, missing the specific intersection of both topics.\n",
    "- **Monolithic Search Backend:** It relies on a single search method and backend, reducing flexibility and the ability to handle multiple data sources.\n",
    "    - Query: \"Latest research in quantum computing.\"\n",
    "    - Issue: The model might only search in a general science database, missing out on specialized quantum computing resources.\n",
    "- **Text Search Limitations:** The model is restricted to simple text queries without the nuances of advanced search features.\n",
    "    - Query: \"what problems did we fix last week\"\n",
    "    - Issue: cannot be answered by a simple text search since documents that contain problem, last week are going to be present at every week.\n",
    "- **Limited Planning Ability:** It fails to consider additional contextual information that could refine the search results.\n",
    "    - Query: \"Tips for first-time Europe travelers.\"\n",
    "    - Issue: The model might provide general travel advice, ignoring the specific context of first-time travelers or European destinations.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2b6dc1c3-90af-4f53-93a5-07ffb3957ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from indexify import IndexifyClient\n",
    "client = IndexifyClient()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "736d511a-991d-43f2-8627-4334ab970a21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Extractor(name=tensorlake/instructor, description=Instructor Extractor, input_params={'properties': {'instructions': {'anyOf': [{'type': 'string'}, {'type': 'null'}], 'default': None, 'title': 'Instructions'}, 'model': {'default': 'gpt-3.5-turbo', 'title': 'Model', 'type': 'string'}, 'schema_bytes': {'format': 'binary', 'title': 'Schema Bytes', 'type': 'string'}, 'system_message': {'anyOf': [{'type': 'string'}, {'type': 'null'}], 'default': None, 'title': 'System Message'}}, 'required': ['schema_bytes'], 'title': 'InputParams', 'type': 'object'}, input_mime_types=['text/plain'], outputs={'model': {'metadata': {'age': {'comment': None, 'type': 'int'}, 'name': {'comment': None, 'type': 'text'}, 'place': {'comment': None, 'type': 'text'}}}})]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.extractors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e6c146f9-6863-406f-ac26-70866cc661fb",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "cannot pickle 'sqlite3.Connection' object",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mbase64\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjson\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m extraction_model \u001b[38;5;241m=\u001b[39m base64\u001b[38;5;241m.\u001b[39mb64encode(\u001b[43mcloudpickle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdumps\u001b[49m\u001b[43m(\u001b[49m\u001b[43mExtraction\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m      5\u001b[0m extraction_model \u001b[38;5;241m=\u001b[39m extraction_model\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      6\u001b[0m input_params\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mschema_bytes\u001b[39m\u001b[38;5;124m'\u001b[39m: extraction_model, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msystem_message\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mYour role is to extract chunks from the following and create a set of topics.\u001b[39m\u001b[38;5;124m'\u001b[39m}\n",
      "File \u001b[0;32m~/Projects/indexify-extractors/ve/lib/python3.12/site-packages/cloudpickle/cloudpickle.py:1479\u001b[0m, in \u001b[0;36mdumps\u001b[0;34m(obj, protocol, buffer_callback)\u001b[0m\n\u001b[1;32m   1477\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m io\u001b[38;5;241m.\u001b[39mBytesIO() \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[1;32m   1478\u001b[0m     cp \u001b[38;5;241m=\u001b[39m Pickler(file, protocol\u001b[38;5;241m=\u001b[39mprotocol, buffer_callback\u001b[38;5;241m=\u001b[39mbuffer_callback)\n\u001b[0;32m-> 1479\u001b[0m     \u001b[43mcp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1480\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m file\u001b[38;5;241m.\u001b[39mgetvalue()\n",
      "File \u001b[0;32m~/Projects/indexify-extractors/ve/lib/python3.12/site-packages/cloudpickle/cloudpickle.py:1245\u001b[0m, in \u001b[0;36mPickler.dump\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m   1243\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdump\u001b[39m(\u001b[38;5;28mself\u001b[39m, obj):\n\u001b[1;32m   1244\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1245\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1246\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1247\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(e\u001b[38;5;241m.\u001b[39margs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrecursion\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m e\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;241m0\u001b[39m]:\n",
      "\u001b[0;31mTypeError\u001b[0m: cannot pickle 'sqlite3.Connection' object"
     ]
    }
   ],
   "source": [
    "    import cloudpickle\n",
    "    import base64\n",
    "    import json\n",
    "    extraction_model = base64.b64encode(cloudpickle.dumps(Extraction))\n",
    "    extraction_model = extraction_model.decode('utf-8')\n",
    "    input_params={'schema_bytes': extraction_model, 'system_message': 'Your role is to extract chunks from the following and create a set of topics.'}\n",
    "    client.add_extraction_policy(extractor='tensorlake/instructor', name=\"extract_chunks\", input_params=input_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "1de2ae1d-666b-45c9-8aba-129f0d9a2f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.add_documents(text_chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba618ef-9af1-4621-bb42-0a2958c42a67",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
